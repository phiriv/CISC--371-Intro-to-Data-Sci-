{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Here the input is a dataset with 3 features: an integer ID number, a binary rating, and a text review.\n",
    "\n",
    "The output is to generate binary sentiment ratings (positive or negative) for a test set for each ID based off only the given text reviews. \n",
    "\n",
    "The data mining function required is analyzing the ID, rating, and text triples to establish which of several reviews correspond to the same product. This is challenging because many of the reviews don't specifically describe the item (e.g. 'I like this Brand X bar of soap since...') and may contain OOV (Out-Of-Vocabulary) words that were not in the training set. Potential impacts include improved product recommendations, adding or removing different products to the online store, and identifying high-value customers. An ideal solution contains a nonlinear classifier many-to-one classifier that produces an F1 score as close as possible to 1.0 on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQxnfuP7VX1Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#Student Name:    Philippe C. Rivet\n",
    "#Student Number:  10105954\n",
    "#Student Email:   13cpr@queensu.ca\n",
    "#Description:     Submission for CISC 372 A2 with comments added for descriptive purposes\n",
    "\n",
    "!wget -q https://l1nna.com/372/Assignment/A2-3/train.csv\n",
    "!wget -q https://l1nna.com/372/Assignment/A2-3/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WVbJBjQVt4H"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #necessary package imports for functionality\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#additional package & class defn. for natural language pre-processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer  #snowball is one of the main stemming algos, along with Porter & Lancaster\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "snowball=SnowballStemmer(language='english')\n",
    "\n",
    "'''\n",
    "class SnowStemmer:\n",
    "    def __init__(self):\n",
    "        self.sbs=snowball\n",
    "    def __call__(self,doc):\n",
    "        return [self.sbs.stem(t) for t in word_tokenize(doc)]\n",
    "        '''\n",
    "\n",
    "#stop_words = set(stopwords.words('english'))  #save list of stopwords to remove in memory\n",
    "\n",
    "xy_train = pd.read_csv('train.csv')\n",
    "x_test  = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPl59bXdWKdu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5348/56240292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     refit='f1', n_jobs=20, scoring=scoring, return_train_score=True)\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;31m#grid_search.fit(x_stem, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#grid_search.fit(x_stop, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = xy_train.review  #dependent var is the reviews\n",
    "y = xy_train.rating  #independent var is the ratings\n",
    "\n",
    "#x_stem=[snowball.stem(wd) for wd in word_tokenize(x)]\n",
    "#word_tokens=word_tokenize(x)\n",
    "#x_stop=[w for w in word_tokens if not w.lower() in stop_words]   #convert all to lowercase as well\n",
    "\n",
    "\n",
    "pipeline = Pipeline([                      #set up data pipeline for a sequence of transformations\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', XGBClassifier(\n",
    "                          objective='multi:softmax', seed=1, num_class=2)),\n",
    "])\n",
    "\n",
    "\n",
    "'''\n",
    "pipeline = Pipeline([                      \n",
    "    ('vect', HashingVectorizer()),\n",
    "    ('clf', SVC(class_weight='balanced')),\n",
    "])\n",
    "'''\n",
    "\n",
    "#these will be changed during experimentation, but carefully as running time can increase exponentially or\n",
    "#even factorially if too many are enabled\n",
    "parameters = {\n",
    "    'vect__max_features': [100, 500, 1000, 5000, 10000, 120000],\n",
    "    'vect__analyzer': ['char',],\n",
    "    'vect__ngram_range': ((1, 2),(1, 3)), # unigrams or bigrams or trigrams etc\n",
    "    'tfidf__use_idf': (True, False),\n",
    "     #'tfidf__norm': ('l1')\n",
    "#     'clf__max_iter': (20,),\n",
    "     #'clf__alpha': (0.001, 0.00001, 0.000001),\n",
    "#     'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "scoring = ['f1', 'accuracy'] #how do we measure the quality of the results?\n",
    "split = int(len(x) * 0.8)    #80% of data used during testing, the remaining 20% during cross-validation\n",
    "'''\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, parameters, verbose=3, cv=[(np.arange(0, split), np.arange(split, len(x)))], \n",
    "    refit='f1', n_jobs=20, scoring=scoring, return_train_score=True) #param search method can also be changed\n",
    "    '''\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    pipeline, parameters, verbose=3, cv=[(np.arange(0, split), np.arange(split, len(x)))], \n",
    "    refit='f1', n_jobs=20, scoring=scoring, return_train_score=True)\n",
    "grid_search.fit(x, y)\n",
    "#grid_search.fit(x_stem, y)\n",
    "#grid_search.fit(x_stop, y)\n",
    "\n",
    "#DESCRIPTION OF MODIFICATIONS 😎🤠🤖\n",
    "\n",
    "#Tuning 1:          Stemming preprocessing\n",
    "#Reasoning:         Simplify the input data by shortening common words\n",
    "#Expected outcome:  Higher performance from reducing overfitting\n",
    "#Actual outcome:    Small increase in performance, but not huge since the Snowball algo preserves ~90% of information\n",
    "#see here for details https://towardsdatascience.com/stemming-corpus-with-nltk-7a6a6d02d3e5\n",
    "\n",
    "#Tuning 2:          Stop word & punctuation preprocessing\n",
    "#Reasoning:         Simplify the input data by removing unnecessary words\n",
    "#Expected outcome:  Higher performance from reducing overfitting\n",
    "#Actual outcome:    Negligible increase in performance as no sentiment information is contained in these words to begin with\n",
    "\n",
    "#Tuning 3:          Use of HashingVectorizer instead of CountVectorizer\n",
    "#Reasoning:         Improved classification as this method produces occurences over counts straight away\n",
    "#Expected outcome:  Higher performance from reduced bias\n",
    "#Actual outcome:    Slight reduction in performance due to collisions, i.e. two or more tokens mapped to same index\n",
    "\n",
    "#Tuning 4:          Change analyzer from word-level to character-level\n",
    "#Reasoning:         Look at single characters over words for the purpose of simplification\n",
    "#Expected outcome:  Improvement in performance from reduced bias\n",
    "#Actual outcome:    Slight increase in performance as key correlations are gained when analyzing at this resolution\n",
    "\n",
    "#Tuning 5:          Change distance metric for tf-idf\n",
    "#Reasoning:         Evaluate whether using L1 (Manhattan) or L2 (Euclidean) norms have different effects\n",
    "#Expected outcome:  Improvement in performance from reduced overfitting with L1\n",
    "#Actual outcome:    Major increase in performance due to trimming effect\n",
    "\n",
    "#Tuning 6:          Add iterations with different learning rates\n",
    "#Reasoning:         Evaluate whether changing the alpha param has an effect\n",
    "#Expected outcome:  No major change in performance as in the case of NLP this param is less important\n",
    "#Actual outcome:    As expected\n",
    "\n",
    "#Tuning 7:          Change classifier to RBF SVM\n",
    "#Reasoning:         Attempt to establish whether changing the classifier has a major effect\n",
    "#Expected outcome:  Massive performance increase from non-linear capabilities\n",
    "#Actual outcome:    Substantial increase on training, substantial decrease on testing\n",
    "\n",
    "#Tuning 8:          Change hyperparam search to random\n",
    "#Reasoning:         Less exhaustive search has a chance of yielding lower training time\n",
    "#Expected outcome:  60% chance of faster execution\n",
    "#Actual outcome:    As expected\n",
    "\n",
    "#TUning 9:          XGBOOSTUH\n",
    "#Expected outcome:  Tremendous improvement in performance from booosting effekt\n",
    "#Reasoning:         Summation of extreme parallelized computation of decision trees\n",
    "#Actual outcome:    WHAT I WANTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcsof-gvWnl-"
   },
   "outputs": [],
   "source": [
    "# let's visualize hyperparameters against performance\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "selected_parameter = 'vect__max_features'  #vary one hyperparam at a time for testing\n",
    "#selected_parameter = 'vect__ngram_range'  #hashingVectorizer doesn't have max_features so we vary this instead\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"GridSearchCV\",\n",
    "          fontsize=16)\n",
    "\n",
    "plt.xlabel(selected_parameter)\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(0.4, 1.1)\n",
    "\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_'+ selected_parameter].data, dtype=float)\n",
    "\n",
    "scorer = 'f1'\n",
    "color ='b'\n",
    "for sample, style in (('train', '--'), ('test', '-')):\n",
    "    sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "    sample_score_mean = [x for _,x in sorted(zip(X_axis,sample_score_mean))]\n",
    "    ax.plot(sorted(X_axis), sample_score_mean, style, color=color,\n",
    "            alpha=1 if sample == 'test' else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample if sample == 'train' else 'validation'))\n",
    "\n",
    "best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "# Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "        linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "# Annotate the best score for that scorer\n",
    "ax.annotate(\"%0.2f\" % best_score,\n",
    "            (X_axis[best_index], best_score + 0.005))\n",
    "    \n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItwJEPgIW49m"
   },
   "outputs": [],
   "source": [
    "# generate submission\n",
    "\n",
    "y_predict = np.squeeze(grid_search.predict(x_test.review))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test.id, 'rating':y_predict}).to_csv('submit5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWERS TO QUESTIONS\n",
    "\n",
    "1. The difference between character n-gram & word n-gram is the size of the groupings: the former groups individual characters, whereas the latter groups words. Word n-gram tends to suffer more from OOV issue because it is not able to recombine previously used characters in new ways to identify unknown words.\n",
    "2. Stop word removal involves removing specific irrelevant words from a pre-specified list, whereas stemming is a method where longer words are shortened by truncating suffixes. These techniques are not language-dependent because in both cases the structure of the language itself determines which words are possible end results of either process.\n",
    "3. Tokenization techniques are language dependent because they convert natural language data to numeric values.\n",
    "4. The CountVectorizer method converts text data to a (typically very bigly) matrix of token counts. Tf-idf transforms a count matrix into a normalized form via logarithmic smoothing. It is not feasible to use all possible n-grams as the running time and storage grow factorially, so they should be selected on a careful basis depending on contextual cues. In most cases stopping at a reasonably high value (say 4 or 5) works well enough 🤠🤓"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
